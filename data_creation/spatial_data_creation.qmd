---
title: "Spatial data creation"
author: "Galen Holt"
format: html
editor: visual
params:
  REBUILD_DATA: FALSE
---

## TO REBUILD

Rebuilding data is done with params. To rebuild, at the terminal run `quarto render data_creation/spatial_data_creation.qmd -P REBUILD_DATA:TRUE` or, to rebuild *everything*, run `quarto render -P REBUILD_DATA:TRUE` (or without the parameters if we want to re-render but not rebuild.)

Set the directory. This got convoluted with the Rproject nested inside quarto, but I think this largely keeps the R self-contained while still allowing Quarto to render. I think I will eventually be able to use `library(HydroBOT)`, but quarto is grumpy about it, so skip for now.

```{r setup}
#| warning: false
#| message: false
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

```

```{r}
library(sf)
library(patchwork)
library(ggplot2)
# library(HydroBOT) works when running interactively, but not when quarto
# rendering. Just use load_all for now.
# Also have to turn off caching and delete _cache directories or it gets really
# upset.
devtools::load_all()
# library(HydroBOT)
```

Set with params. But that doesn't work with interactive running, so deal with that.

```{r}
if ("params" %in% ls()) {
  REBUILD_DATA <- params$REBUILD_DATA
} else {
  REBUILD_DATA <- FALSE
}

```

Trying to be as intentional as possible with the data dirs. `data-raw` is for raw data that is used to create package data, and is hidden when building. I'm differentiating downloaded from not to hopefully avoid deletion issues and make it clearer what `data` is a typical R package data dir, following the conventions of `.rda` files with single objects. And `inst/extdata` is for data that *isn't* in R format (here, shapefiles that we want to share around).

```{r}
datarawdir <- 'data-raw'
dataoutdir <- 'data'
cleanspatialdir <- 'data-raw/cleanspatial'

if (!dir.exists(datarawdir)) {dir.create(datarawdir, recursive = TRUE)} 
if (!dir.exists(dataoutdir)) {dir.create(dataoutdir, recursive = TRUE)} 
if (!dir.exists(cleanspatialdir)) {dir.create(cleanspatialdir, recursive = TRUE)} 
```

## File purpose

Create some clean(er) spatial data for aggregation testing demonstrations. I'm going to get as many files as possible online, and put others files in `data-raw` for now, and output clean versions to `data`, following R package data conventions. But set a 'datadir', so we can change that easily to somewhere else if desired.

Define a dataload function for download and unzip of shapefiles- doing this here rather than in R/ because this is not really part of the package, but the initial dev building. And it can know about existing dirs

```{r}
data_load <- function(dirname, datadir, sourceurl,  
                      existing_dirs = list.files(datadir)) {

  if (!(dirname %in% existing_dirs)) {
      if (!dir.exists(file.path(datadir, dirname))) {
        dir.create(file.path(datadir, dirname), recursive = TRUE)
        }
    zippath <- file.path(datadir, paste0(dirname, '.zip'))
    
    # Curl only tested wiht ftp, but probably works everywhere. download.file failing with ftp
    if (grepl('ftp:', sourceurl)) {
      curl::curl_download(sourceurl, destfile = zippath)
    }
    download.file(sourceurl, destfile = zippath)
    
    unzip(zippath, exdir = file.path(datadir, dirname))
    
    file.remove(zippath)
  }
}
```

## Basin

First, read in the basin. This file straight from MDBA. Currently a download link to get data into data-raw, but can later just point to an internal location to avoid putting the data in the package.

File from

```{r}

data_load('downloaded/mdb_boundary', datarawdir, "https://data.gov.au/data/dataset/4ede9aed-5620-47db-a72b-0b3aa0a3ced0/resource/8a6d889d-723b-492d-8c12-b8b0d1ba4b5a/download/sworkingadhocjobsj4430dataoutputsmdb_boundarymdb_boundary.zip")

```

```{r}
basin <- read_sf(file.path(datarawdir,  'downloaded', 
                           'mdb_boundary',
                           'mdb_boundary.shp'))
```

## Resource planning areas

Also from MDBA- do this with a link too

```{r}
data_load('downloaded/sw_rpa', datarawdir, "https://data.gov.au/data/dataset/7b0c274f-7f12-4062-9e54-5b8227ca20c4/resource/8369c483-975c-4f54-8eda-a6fbd0ea89ad/download/sworkingadhocjobsp3827dataoutputsupdate-april-2019sw-wrpasurface-water-water-resource-plan-areas.zip")

```

```{r}
rpa <- read_sf(file.path(datarawdir, 'downloaded',
                         'sw_rpa', 
                         'Surface Water Water Resource Plan Areas.shp')) |> 
  st_make_valid()
```

After a bunch of speed testing, it became apparent `rpa` is a monster (1.4 million + vertices).

```{r}
print(vertcount(rpa))
```

Simplify it. The obvious route `sf::st_simplify` is easiest, but doesn't preserve touchingness of adjoining polygons. That may not actually matter if we then scale up more, but it will if this is the final level. So use `rmapshaper::ms_simplify()` (https://github.com/r-spatial/sf/issues/1011) to set up a default polygon set and keep adjoining topology. For now, just pick a level crudely. `sf::st_simplify` does the simplification in meters, but `ms_simplify` is in proportion. A bit harder to judge. Maybe 0.01? The default of 0.05?

```{r}
resource_plan_areas <- rpa |> 
  rmapshaper::ms_simplify(keep = 0.01)

print(vertcount(resource_plan_areas))
```

That's not obviously oversmoothed, so should be fine for testing. If this becomes canonical, we should think more about it.

```{r}
ggplot(resource_plan_areas) +
  geom_sf(aes(fill = SWWRPANAME), show.legend = FALSE) +
  geom_sf_label(aes(label = SWWRPANAME), size = 3, label.padding = unit(0.1, 'lines')) + 
  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')
```

## SDL units

Also from MDBA- do this with a link too

```{r}
data_load('downloaded/sw_sdl', datarawdir, "https://data.gov.au/data/dataset/4afa3227-8557-4bb6-944a-6494b28ae160/resource/988ae947-f4dc-4f75-9773-536df71b03ad/download/sworkingadhocjobsp3827dataoutputsupdate-january-2019surface-water-sdl-resource-units.zip")

```

```{r}
sdls <- read_sf(file.path(datarawdir, 'downloaded',
                         'sw_sdl', 
                         'Surface Water SDL Resource Units.shp')) |> 
  st_make_valid()
```

The `sdl` polygons are also monsters (1.4 million + vertices).

```{r}
print(vertcount(sdls))
```

again, too many vertices

```{r}
sdl_units <- sdls |> 
  rmapshaper::ms_simplify(keep = 0.01)

print(vertcount(sdl_units))
```

And a plot. Too many units to name so kill the legend

```{r}
ggplot(sdl_units) +
  geom_sf(aes(fill = SWSDLName), show.legend = FALSE) +
  geom_sf_label(aes(label = SWSDLName), size = 3, label.padding = unit(0.1, 'lines')) + 
  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')
```

The 'Macquarie' file I got from David R. is the Macquarie-Castlereagh there, so just use this to avoid having even more files to lug around.

Change the long dashes to short

```{r}
sdl_units <- sdl_units |> 
  dplyr::mutate(SWSDLName = stringr::str_replace_all(SWSDLName, 'â€“', '-'))
```

## Planning units

These are what the EWRs are indexed to. I currently have a geoJSON from Ben Bradshaw for NSW, hopefully will soon get a complete original shapefile. These are lat/long, put in same crs as others.

```{r}
PUs <- read_sf(file.path(datarawdir, 'planning_units',
                         'EAFDBrowserBackup22v0Dlatlon.json')) |> 
  st_make_valid() |> 
  st_set_crs(4326) |> 
  st_transform(crs = st_crs(basin))
```

How many vertices is that?

```{r}
print(vertcount(PUs))
```

That's a lot, but there are also a lot more polygons here. I'll still smooth a bit, because it makes the spatial operations so much smoother.

```{r}
planning_units <- PUs |> 
  rmapshaper::ms_simplify(keep = 0.01)

print(vertcount(planning_units))
```

We want the names to match the ewr table

The names are close but not always the same.

One issue is whether or not 'to' is capitalised, there are ampersands, and other strange inconsistencies. It's hard to check because the PUs are only NSW, but I'll do what I can.

```{r}
planning_units <- planning_units |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name, ' To ', ' to ')) |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name, '&', 'and')) |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name, '\\(', '')) |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name, '\\)', '')) |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name,
                                                   'Murrumbidgee River - ',
                                                   'Murrumbidgee River_')) |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name,
                                                   'Upper Murrumbidgee River - ',
                                                   'Upper Murrumbidgee River_')) |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name,
                                                   'Tumut River Below Blowering Dam',
                                                   'Tumut River below Blowering Dam')) |> 
  dplyr::mutate(PU_Name = stringr::str_replace_all(PU_Name,
                                                   'Cambells Creek',
                                                   'Campbells Creek'))


```

```{r}
ewrtab <- get_ewr_table()
unique(ewrtab$PlanningUnitName) %in% unique(planning_units$PU_Name)
```

These are the EWR PUs that are not in the polygons. Most are not in NSW, so assume they're not in this set of polygons.

```{r}
unique(ewrtab$PlanningUnitName[!ewrtab$PlanningUnitName %in% unique(planning_units$PU_Name)]) |> sort()
```

The IDs do not seem to exist in the json- there's no 215 in the planning_units

```{r}
ewrtab |> dplyr::filter(PlanningUnitName == "Mungindi to Boomi River Confluence") |> dplyr::slice(1)
planning_units |> dplyr::filter(PU_Name == "Mungindi to Boomi River Confluence") |> dplyr::slice(1)
```

The LTWP names are also inconsistent

```{r}
unique(ewrtab$LTWPShortName) |> sort()
```

```{r}
unique(planning_units$LTWP_Area) |> sort()
```

We can straighten that up fairly easily

```{r}
planning_units <- planning_units |> 
  dplyr::mutate(LTWP_Area = stringr::str_replace_all(LTWP_Area,
                                                     'Barwon Darling',
                                                     'Barwon-Darling')) |> 
  dplyr::mutate(LTWP_Area = stringr::str_replace_all(LTWP_Area,
                                                     'Intersecting Streams',
                                                     'Intersecting_Streams')) |> dplyr::mutate(LTWP_Area = stringr::str_replace_all(LTWP_Area,
                                                     'Macquarie',
                                                     'Macquarie-Castlereagh')) |> 
  dplyr::mutate(LTWP_Area = stringr::str_replace_all(LTWP_Area,
                                                     'Murray',
                                                     'Murray Lower Darling'))
```

```{r}
unique(planning_units$LTWP_Area) %in% unique(ewrtab$LTWPShortName)
```

But, there are duplicates that arise from the SWWRPANAME

```{r}
duplicate_plan <- planning_units |> 
  dplyr::group_by(PU_Name) |> 
  dplyr::filter(dplyr::n() > 1) |> 
  dplyr::arrange(PU_Name)

duplicate_plan
```

```{r}
ggplot() +
  geom_sf(data = resource_plan_areas, mapping = aes(color = SWWRPANAME), fill = 'grey90') +
  geom_sf(data = duplicate_plan, mapping = aes(fill = PU_Name)) + 
  guides(fill = 'none')
  # theme(legend.position = 'none')
```

We don't actually want these to be distinct; the splitting is great, but we want to split based on the polygons we want, not necessarily the resource plan areas. So, let's put the planning units back together.

First, fix a state placeholder that yields more duplications

```{r}
planning_units <- planning_units |> 
  dplyr::mutate(PU_Name = ifelse(PU_Name == 'To be determined', 
                paste0(PU_Name, STATE), 
                PU_Name))
```

Now do the unions

```{r}
planning_units <- planning_units |> 
  dplyr::group_by(PU_Name) |> 
  dplyr::summarise(LTWP_Area = unique(LTWP_Area),
                   PU_Region = unique(PU_Region),
                   PU_Code = unique(PU_Code),
                   STATE = unique(STATE),
                   geometry = st_union(geometry)) |> 
  dplyr::ungroup()
```

```{r}
duplicate_plan2 <- planning_units |> 
  dplyr::group_by(PU_Name) |> 
  dplyr::filter(dplyr::n() > 1) |> 
  dplyr::arrange(PU_Name)

duplicate_plan2
```

Let's make the columns that do match have the same name. We also want a planning_unit_name column to match the cleaned names the toolkit uses. That's a bit silly, but it will be good to be able to join to ewrs or toolkit outputs.

```{r}
planning_units <- planning_units |> 
  # dplyr::select(-id) |> # Nothing in there
  dplyr::rename(PlanningUnitName = PU_Name,
                LTWPShortName = LTWP_Area) |> 
  dplyr::mutate(planning_unit_name = PlanningUnitName)
```

```{r}
ggplot(planning_units) +
  geom_sf(aes(fill = PlanningUnitName), show.legend = FALSE) + 
  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')
```

## Gauges

There are gauge locations from the MDBA gis zip (HydrologicIndicatorSites) and BOM (included in EWR tool- bom_gauge_data). I can get the hydrologic indicator sites online, but the only source I can find for the BOM locations is a manual link at <http://www.bom.gov.au/waterdata/>. It's a csv (I got from old version of EWR), so I'm just going to include it for now.

```{r}
data_load('downloaded/hyd_ind_sites', datarawdir, "https://data.gov.au/data/dataset/2fbf1d73-712d-4ddb-9d2a-8b4119697525/resource/e507d6a8-bcf1-496a-bfb4-79769c02eced/download/sworkingadhocjobsj4068dataoutputshydrologicindicatorsites_point.zip")

```

```{r}
hydind <- read_sf(file.path(datarawdir, 'downloaded',
                            "hyd_ind_sites",
                            "HydrologicIndicatorSites_Point.shp"))

bomg <- readr::read_csv(file.path(datarawdir, 'bom_gauge_locations', 'bom_gauge_data.csv'))
```

Make BOM geographic

The barrage flow is not available from any database I can find easily, but it is available from [SA water](https://water.data.sa.gov.au/Data/Location/Summary/Location/A4261002/Interval/Latest).

```{r}
barrage_gauge <- tibble::tibble(`site name` = "River Murray Calculated Barrage Flow",
                        `gauge number` = "A4261002", 
                        owner = "SA - Department of Environment, Water and Natural Resources",
                        lat = "-35.57316", lon = "138.98371")

bomg <- dplyr::bind_rows(bomg, barrage_gauge)
```

```{r}
bomg <- bomg |> 
    dplyr::rename(site = 'site name', gauge = 'gauge number') |>
    # lat an long come in as chr because there is a line for 'undefined'
    dplyr::filter(site != 'undefined') |>
    dplyr::mutate(lat = as.numeric(lat),
           lon = as.numeric(lon)) |> 
  st_as_sf(coords = c('lon', 'lat'), crs = 4326) |> 
  st_transform(crs = st_crs(basin))
```

How do they compare?

MDBA locations

```{r}
ggplot() + 
  geom_sf(data = basin) +
  geom_sf(data = hydind)
```

BOM are all of Australia

```{r}

ggplot() + 
  geom_sf(data = basin) +
  geom_sf(data = bomg)
```

are the boms in the basin the same as the mdbas?

```{r}
inbasin <- st_intersects(basin, bomg)
# That's a sparse matrix of polygon-point pairs, but since we only have one polygon, can unlist
bom_basin_gauges <- bomg[unlist(inbasin), ]
nrow(hydind)
nrow(bom_basin_gauges)
```

Quite clearly more in bom, but plot

```{r}
ggplot() +
  geom_sf(data = basin) +
  geom_sf(data = bom_basin_gauges, color = 'dodgerblue') + 
  geom_sf(data = hydind, color = 'firebrick', alpha = 0.5)
```

Worth thinking about which to use by default, but let's allow feeding it a path to whatever and using it. I'm going to move that bom dataset into `data` too, and probably use it by default since it seems to be what the EWR is using. Easy to change later.

## Catchments

Get these from CEWO, there are other sources but I've used a version of this before.

```{r}
data_load('downloaded/cewo_valleys', datarawdir, "https://data.gov.au/data/dataset/75910bc5-6c3e-40e8-9c8a-1e895274badb/resource/a7053ee7-8e20-4f2c-b594-cb88c6ed9406/download/cewo_mdb_valleys.shp.zip")
```

A bit of cleanup to match crs to the MDBA data, and discard unused cols.

```{r}
cewo_valleys <- read_sf(dsn = file.path(datarawdir, 'downloaded',
                                        'cewo_valleys',
                                        'CEWO_MDB_Valleys.shp')) |>
  st_transform(st_crs(rpa)) |>
  st_make_valid() |>
  dplyr::select(ValleyName, ValleyID, ValleyCode) |>
  dplyr::filter(ValleyName != 'Northern Unregulated')

```

Plot that, make sure it worked

```{r}
ggplot(cewo_valleys) +
  geom_sf(aes(fill = ValleyName))
```

## River lines (geofabric)

The toolkit does not (at present) use river lines for processing, but they are very useful for visualisation.

```{r}
data_load('downloaded/SH_Network_GDB', 
          datarawdir,
          "ftp://ftp.bom.gov.au/anon/home/geofabric/SH_Network_GDB_V3_3.zip")
```

```{r}
st_layers(file.path('data-raw/downloaded', 'SH_Network_GDB', 'SH_Network.gdb'))
```

```{r}
rivers <- st_read(file.path('data-raw/downloaded', 'SH_Network_GDB', 'SH_Network.gdb'), layer = 'AHGFNetworkStream')
```

Subset to major rivers

```{r}
major_rivers <- rivers |> 
  dplyr::filter(Hierarchy == 'Major')
```

Already in the same crs as the others

```{r}
st_crs(major_rivers) == st_crs(basin)
```

Clip to the basin

```{r}
basin_fabric_rivers_full <- major_rivers |> 
  st_intersection(basin)
```

We should probably smooth that out again

```{r}
print(vertcount(basin_fabric_rivers_full))
```

```{r}
basin_fabric_rivers <- basin_fabric_rivers_full |> 
  rmapshaper::ms_simplify(keep = 0.001, drop_null_geometries = TRUE, 
                          method = 'vis')

print(vertcount(basin_fabric_rivers))
```

Change the Shape to geometry for consistency

```{r}
basin_fabric_rivers <- basin_fabric_rivers |> 
  dplyr::rename(geometry = Shape) |> 
  dplyr::select(Name)
```

```{r}
ggplot() +
  geom_sf(data = basin, fill = 'grey96') +
  geom_sf(data = basin_fabric_rivers, aes(color = log(UpstrGeoLn))) +
  scale_color_viridis_c() +
  theme_void() +
  theme(legend.position = 'none')
```

## River lines (HydroRIVERS)

```{r}
data_load('downloaded/HydroRIVERS', 
          datarawdir,
          "https://data.hydrosheds.org/file/HydroRIVERS/HydroRIVERS_v10_au.gdb.zip")
```

```{r}
st_layers(file.path('data-raw/downloaded', 'HydroRIVERS',
                    'HydroRIVERS_v10_au.gdb'))
```

```{r}
hydro_rivers <- st_read(file.path('data-raw/downloaded', 'HydroRIVERS',
                    'HydroRIVERS_v10_au.gdb'),
                  layer = 'HydroRIVERS_v10_au')
```

```{r}
basin_hydro_rivers_full <- hydro_rivers |> 
  st_transform(crs = st_crs(basin)) |> 
  st_intersection(basin)
```

```{r}
basin_hydro_rivers <- basin_hydro_rivers_full |> 
  rmapshaper::ms_simplify(keep = 0.001, drop_null_geometries = TRUE, 
                          method = 'vis')

print(vertcount(basin_hydro_rivers))
```

```{r}
basin_hydro_rivers <- basin_hydro_rivers |> 
  dplyr::rename(geometry = Shape) |> 
  dplyr::filter(ORD_STRA >= 3) |> 
  dplyr::select(ORD_STRA, DIST_UP_KM, geometry)
```

```{r}
ggplot() +
  geom_sf(data = basin, fill = 'grey96') +
  geom_sf(data = basin_hydro_rivers |> dplyr::filter(ORD_STRA >= 3), 
          aes(color = log(DIST_UP_KM))) +
  scale_color_viridis_c() +
  theme_void() +
  theme(legend.position = 'none')
```

Use this one as `basin_rivers` to provide- it's cleaner.

```{r}
basin_rivers <- basin_hydro_rivers
```

# Save clean files

No changes to basin, simplified rpa, bom gauges in the basin, and ltim valleys extracted from anae.

I *could* save these objects as the sf objects in `.rda` files, as would be typical for an R package. But `.shp` is going to be easier for other programs to read if we want to share them (and we do).

*Only do this if we want to rebuild*- the `data/` directory is tracked, so don't do this willy-nilly.

```{r}

if (REBUILD_DATA) {
  # The shapefiles to /inst/extdata/
  st_write(basin, 
           file.path(cleanspatialdir, 'basin.shp'), 
           append = FALSE)
  st_write(resource_plan_areas, 
           file.path(cleanspatialdir, 'resource_plan.shp'), 
           append = FALSE)
  st_write(sdl_units, 
           file.path(cleanspatialdir, 'sdl_units.shp'), 
           append = FALSE)
  st_write(bom_basin_gauges, 
           file.path(cleanspatialdir, 'bom_gauges.shp'), 
           append = FALSE)
  st_write(cewo_valleys, 
           file.path(cleanspatialdir, 'cewo_valleys.shp'), 
           append = FALSE)
    st_write(planning_units, 
           file.path(cleanspatialdir, 'planning_units.shp'), 
           append = FALSE)
    st_write(basin_rivers, 
           file.path(cleanspatialdir, 'basin_rivers.shp'), 
           append = FALSE)

  
  # The R objects to /data/
  usethis::use_data(basin, overwrite = TRUE)
  usethis::use_data(resource_plan_areas, overwrite = TRUE)
  usethis::use_data(sdl_units, overwrite = TRUE)
  usethis::use_data(bom_basin_gauges, overwrite = TRUE)
  usethis::use_data(cewo_valleys, overwrite = TRUE)
  usethis::use_data(planning_units, overwrite = TRUE)
  usethis::use_data(basin_rivers, overwrite = TRUE)

}

```
