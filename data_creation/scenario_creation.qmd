---
title: Generate test gauge data
author: Galen Holt
format:
  html:
    df-print: paged
editor: visual
params:
  REBUILD_DATA: FALSE
---

```{r setup}
#| warning: false
#| message: false
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())

```

```{r}
#| warning: false
#| message: false

# library(tidyverse)
library(ggplot2)
library(sf)
library(reticulate) # Not strictly necessary but allows easier referencing of objects
library(hydrogauge)
library(dplyr)
# library(HydroBOT) works when running interactively, but not when quarto
# rendering. Just use load_all for now.
# Also have to turn off caching and delete _cache directories or it gets really
# upset.
devtools::load_all()
# library(HydroBOT)
```

## TO REBUILD

Rebuilding data is done with params. To rebuild, at the terminal in HydroBOT run `quarto render data_creation/scenario_creation.qmd -P REBUILD_DATA:TRUE`. or, to rebuild *everything*, run `quarto render -P REBUILD_DATA:TRUE` (or without the parameters if we want to re-render but not rebuild.)

## Dev note

In general, we'll have to be careful about mixing R and Python in the package to ensure users have the right environments. Here, though, we assume that this file is only run by devs to create package data, and that these devs have access to the full git repo with the Python env, and so we can be a bit more in control about python envs and locations.

I originally built this in Python ([scenario_creation.qmd](data_creation/scenario_creation.qmd)), but the only bit that *needs* to be in python is the gauge puller code. I'm adding this version as an example of how to mix R and python code chunks in similar situations where we only need a bit of code from the other language.

As long as the venv is in the base project directory, {reticulate} seems to find it. Otherwise, need to tell it where it is with `reticulate::use_virtualenv`. *Now that we have nested directories, I set the RETICULATE_PYTHON env variable in* `.Rprofile` .

I'm leaving the `=` alone rather than changing to `<-`.

Because I've now changed the code to save the `sf` objects as `.rda` files in `data/`, I could skip all the shapefile reading below. But to keep consistent with the python version, which will need to read `.shp` files, I'll do that here too.

None of this actually ends up saving to `data/` because the point is to generate test data that looks like the real data we'd use on input, which currently is csv. So we save all the generated data here to `inst/extdata/`.

# Creation of test data

We need to create test data for several scenarios for several gauges. This finds the gauges to pull based on location, gets the data, and modifies it into scenarios. The first case study is just the Macquarie, but I think I'd also like some additional catchments so the spatial aggregation demonstrations are more interesting. Is there any reason not to do that? Not really.

The shapefiles used to see what we're doing and do the selecting were produced with `data_creation/spatial_data_creation.qmd`.

I output clean test data to `inst/extdata`, following R package data conventions. But set a 'datadir', so we can change that easily to somewhere else if desired.

First, do we actually want to rebuild the `data/` directory, or just use this to investigate and document what happened?

Set with params. But that doesn't work with interactive running, so deal with that.

```{r}
if ("params" %in% ls()) {
  REBUILD_DATA <- params$REBUILD_DATA
} else {
  REBUILD_DATA <- FALSE
}
```

Set the data dir to make that easy to change.

```{r}
datarawdir <- 'data-raw'
dataoutdir <- 'data'
dataextdir <- 'inst/extdata'
```

## Read in gauges

Use the bom gauges, since this is what were contained in the EWR tool.

```{r}
gaugegeo = read_sf(file.path(dataextdir, 'bom_gauges.shp'))
```

## Read in polygons

I'll read in a few options, and then choose- basin, resource plan areas, sdl areas (which is where David got the Macquarie polygon), and catchments. I have cut out the testing of David's single polygon- it is the Macquarie-Castlereagh in the sdls.

```{r}
basin = read_sf(file.path(dataextdir, 'basin.shp'))
rps = read_sf(file.path(dataextdir, 'resource_plan.shp'))
sdl = read_sf(file.path(dataextdir, 'sdl_units.shp'))
ltv = read_sf(file.path(dataextdir, 'cewo_valleys.shp'))
```

crs all match from the creation.

### Plot the polygons and data checks

Basin

```{r}
ggplot(basin) + geom_sf(fill = 'powderblue')
```

Resource plan areas

```{r}
ggplot(rps) + geom_sf(aes(fill = SWWRPANAME), show.legend = FALSE) +
  geom_sf_label(aes(label = SWWRPANAME), size = 3, label.padding = unit(0.1, 'lines')) + 
  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')
```

These have 'SW' codes

```{r}
rps
```

SDL plan areas

```{r}
ggplot(sdl) + geom_sf(aes(fill = SWSDLName), show.legend = FALSE) +
  geom_sf_label(aes(label = SWSDLName), size = 3, label.padding = unit(0.1, 'lines')) + 
  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')
```

These have 'SS' codes.

```{r}
sdl
```

Catchments

```{r}
ggplot(ltv) + geom_sf(aes(fill = ValleyName), show.legend = FALSE) +
  geom_sf_label(aes(label = ValleyName), size = 3, label.padding = unit(0.1, 'lines')) + 
  colorspace::scale_fill_discrete_qualitative(palette = 'Set2')
```

```{r}
ltv
```

## Cut to demo polygons

The Macquarie shapefile from David is the Macquarie-Castlereagh from `rps`. I want a few catchments to play with for demos, so let's use the Macquarie, Castlereagh, Namoi, Lachlan. That might be a LOT of gauges, though.

```{r}
catch_demo = rps |> 
  dplyr::filter(SWWRPANAME %in% c('Macquarie-Castlereagh', 'Lachlan', 'Namoi'))
```

```{r}
(ggplot(catch_demo) + geom_sf(aes(fill = SWWRPANAME)) +
scale_fill_brewer(type = 'qual', palette = 8))
```

## Get relevant gauges

### Cut to the polygons

Cut the gaugegeo from the whole country to just those four catchments

```{r}
demo_gauges = st_intersection(gaugegeo, catch_demo)
```

How many are we talking?

```{r}
demo_gauges |> nrow()
```

295 rows is a lot, but unlikely they'll all be in the EWR.

### Extract their names

To feed to the gauge extractor, we need their gauge numbers.

```{r}
gaugenums <- demo_gauges$gauge
```

## Get the values

We can feed lists to `gg.gauge_pull`, so can feed it that way. We might *want* to loop for parallelising extraction or modifications, but the real scenarios won't be made this way anyway, so not worth it here.

What time span do we want? 10 years to start

```{r}
starttime = lubridate::ymd(20100101)
endtime = lubridate::ymd(20191231)
```

How many are actually in the EWR tool?

```{r, echo = F}
ewrgauges <- HydroBOT::get_ewr_gauges()
```

What are those gauges, and which are in both the ewr and the desired catchments?

```{r}
ewr_demo_gauges <- intersect(ewrgauges$gauge,gaugenums)
length(ewr_demo_gauges)
```

47 seems ok. Let's go with that.

### Get all the gauge data

```{r}
demo_levs <- hydrogauge::get_ts_traces(portal = 'NSW', 
                                      site_list = ewr_demo_gauges,
                                      var_list = c("141","100"), #flow and level
                                      start_time = starttime,
                                      end_time = endtime)
```

### Map the gauges

```{r}
# gaugegeo <- gaugegeo |> 
#   dplyr::rename(gauge = `gauge number`)
demo_geo <- gaugegeo |> dplyr::filter(gauge %in% demo_levs$site)
```

Looks reasonable. Probably overkill for testing, but can do a cut down version too.

```{r}
(ggplot() + 
geom_sf(data = basin, fill = 'lightsteelblue') +
geom_sf(data = catch_demo, mapping = aes(fill = SWWRPANAME)) +
geom_sf(data = demo_geo, color = 'black') +
scale_fill_brewer(type = 'qual', palette = 8))
```

## Make test scenarios

### Demo scenarios

For now, the test scenarios are just multiplying by 4 or 0.25 to give something to work with. This section could easily be modified for other simple scenarios.

```{r}
# for larger runs in ~/HydroBOT/inst/extdata/testscenarios
base <- demo_levs
down4 <- demo_levs
up4 <- demo_levs

down4$value = down4$value * 0.25
up4$value = up4$value * 4
```

### Smaller set

I also want a very limited set of test data that runs faster through the EWR tool and the rest of HydroBOT. It still needs to have \> 1 gauge in \> 1 catchment though. Let's do three gauges in two catchments, 5 years.

Which gauges?

```{r}
# for smaller runs in ~/HydroBOT/inst/extdata/testsmall

# get gauges for smaller set

mac = rps |> 
  dplyr::filter(SWWRPANAME %in% c('Macquarie-Castlereagh'))
lach = rps |> 
  dplyr::filter(SWWRPANAME %in% 'Lachlan')

macgauge = st_intersection(demo_geo, mac)
lachgauge = st_intersection(demo_geo, lach)
macsub = macgauge |> 
  dplyr::filter(gauge %in% c('421004', '421001', '421011'))
lachsub = lachgauge |> 
  dplyr::filter(gauge %in% c('412005', '412002', '412038'))

minigauge = dplyr::bind_rows(macsub, lachsub)

minigauges = unique(minigauge$gauge)

```

```{r}

# restrict dates
start15 = lubridate::ymd(20150101)

# function to subset gauages and date window
restrict <- function(x){
  x <- x |> filter(x$time > start15)
  x <- x |> filter(x$site %in% minigauges)
}

demo_levs_smaller <- restrict(demo_levs)

base_smaller <- demo_levs_smaller
down4_smaller <- demo_levs_smaller
up4_smaller <- demo_levs_smaller

down4_smaller$value = down4_smaller$value * 0.25
up4_smaller$value = up4_smaller$value * 4
```

### Format the data to 'Standard Time Series'

```{r}
make_ewrformat <- function(x) {
  x <- x |> 
    dplyr::mutate(gauge = dplyr::case_when(
      units %in% c('Megalitres/Day') ~ paste0(site, '_flow'),
      units %in% c('m', 'Res. Level AHD', 'Metres') ~ paste0(site, '_level'),
      .default = paste0(site, '_FAIL'))
      ) |> 
    dplyr::select(Date = time, gauge, value) |> 
    tidyr::pivot_wider(id_cols = Date, names_from = gauge, values_from = value) |> 
    dplyr::mutate(Date = as.Date(Date))
  # unnest columns converted to list columns
  cols <- colnames(x[,-1])
  x <- tidyr::unnest(x,cols=cols)
  return(x)
}


```

Create clean dataframes to save.

```{r}
#try add in tidyr::unnest() here to unlist columns

#just do lapply here over all long and smaller 

base <- make_ewrformat(base)
down4 <- make_ewrformat(down4)
up4 <- make_ewrformat(up4) 

base_smaller <- make_ewrformat(base_smaller)
down4_smaller <- make_ewrformat(down4_smaller)
up4_smaller <- make_ewrformat(up4_smaller) 

```

### **Remove duplicates**

For a specific gauage causing duplicate rows with zero level reading and non-zero level reading on same date. Take the non-zero level reading.

```{r}
remove_doubles <- function(x){
    x %>%
    group_by(Date) %>%
    filter(!(duplicated(Date) & `412038_level` == 0))
}

df_list <- list(base,
                down4,
                up4,
                base_smaller,
                down4_smaller,
                up4_smaller)

filtered_df_list <- lapply(df_list,remove_doubles)

base <- filtered_df_list[[1]]
down4 <- filtered_df_list[[2]]
up4 <- filtered_df_list[[3]]
base_smaller <- filtered_df_list[[4]]
down4_smaller <- filtered_df_list[[5]]
up4_smaller <- filtered_df_list[[6]]
```

### Save the output

```{r}
# Create directories

scenenames = c('base', 'up4', 'down4')

# # the full scenarios
# for (x in scenenames) {
#       scenedir = file.path(dataextdir, 'testscenarios', x)
#     if (!dir.exists(scenedir)) {
#       dir.create(scenedir, recursive = TRUE)
#     }
#         
# }


# a minimal set
for (x in scenenames) {
      scenedirS = file.path(dataextdir, 'testsmall', 'hydrographs', x)
    if (!dir.exists(scenedirS)) {
      dir.create(scenedirS, recursive = TRUE)
    }
}


```

```{r}
(ggplot() + 
geom_sf(data = basin, fill = 'lightsteelblue') +
geom_sf(data = catch_demo, mapping = aes(fill = SWWRPANAME)) +
geom_sf(data = minigauge, color = 'black') +
scale_fill_brewer(type = 'qual', palette = 8))
```

### **Export data**

Save. could do this above easily enough, but getting lots of dots and hard to read.

```{r}
# export larger scenarios 

# if (REBUILD_DATA) {
#   readr::write_csv(base, file.path(dataextdir, 'testscenarios', 'base', 'base.csv'))
#   readr::write_csv(up4, file.path(dataextdir, 'testscenarios', 'up4', 'up4.csv'))
#   readr::write_csv(down4, file.path(dataextdir, 'testscenarios', 'down4', 'down4.csv'))
#   } 
```

Write out the scenarios, including metadata to check that that is working (and that we're not picking up extra files in some of the crawling functions).

```{r}
# export smaller scenarios

if (REBUILD_DATA) {

    base_smaller |> 
      readr::write_csv(file.path(dataextdir, 'testsmall', 'hydrographs', 'base', 'base.csv'))
    
    down4_smaller |>  
      readr::write_csv(file.path(dataextdir, 'testsmall', 'hydrographs', 'down4', 'down4.csv'))
    
    up4_smaller |>   
      readr::write_csv(file.path(dataextdir, 'testsmall', 'hydrographs', 'up4', 'up4.csv'))
    
    # make JSON metadata
    jsonlite::write_json(list(scenario_name = 'base', 
                            flow_multiplier = 1), 
                       path = file.path(dataextdir, 'testsmall', 
                                        'hydrographs', 'base', 'base.json'))
    
    jsonlite::write_json(list(scenario_name = 'down4', 
                            flow_multiplier = 0.25), 
                       path = file.path(dataextdir, 'testsmall', 
                                        'hydrographs', 'down4', 'down4.json'))
    
    jsonlite::write_json(list(scenario_name = 'up4', 
                            flow_multiplier = 4), 
                       path = file.path(dataextdir, 'testsmall', 
                                        'hydrographs', 'up4', 'up4.json'))
}
    

```

And all the scenario metadata- it might come in like this and we need to be robust

```{r}
scenario_meta <- list(scenario_name = c('down4', 'base', 'up4'),
                      flow_multiplier = c(0.25, 1, 4))
# I don't know the format we'll be using, but this works to create yaml metadata
hydro_dir <- file.path(dataextdir, 'testsmall', 'hydrographs')
yaml::write_yaml(scenario_meta, 
                 file = file.path(hydro_dir, 'scenario_metadata.yml'))
# and this does the same with JSON
jsonlite::write_json(scenario_meta, 
                     path = file.path(hydro_dir, 'scenario_metadata.json'))
```
