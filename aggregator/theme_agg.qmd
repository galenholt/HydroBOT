---
title: "Theme aggregation"
author: "Galen Holt"
format:
  html:
    df-print: paged
editor: visual
---

```{r setup}
#| warning: false
#| message: false

# knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())


library(tidyverse)
library(doFuture)
library(foreach)
library(sf) # not stricly needed for theme agg, but use it for demo

```

## Overview

We need to aggregate outcomes along the theme dimension. For example, we might want to combine EWR pass/fails into the proportion of EWRs contributing to a proximate objective that passed, and then translate that into outcomes for 5-year targets or Waterbirds, etc.

The input data is thus the data coming out of the theme modules (e.g. EWR tool), which is then aggregated. The relationships that define the aggregations are the same as those defining the causal networks- these map finer-scale groups to coarser. Thus, we need to access those relationships (and the `make_edges` function). I'm using `source` and `load` for now, but if causal becomes a package, then we can just include it in the `imports` for this.

```{r}
# source causal functions. Really should import make_edges
causefiles <- list.files(file.path('R', 'causal_networks'))
for (s in 1:length(causefiles)) {
  source(file.path('R', 'causal_networks', causefiles[s])) 
}
```

Finally, we need to specify the function(s) to use to do the aggregation at each step.

```{r}
aggfiles <- list.files(file.path('R', 'aggregator'))
for (s in 1:length(aggfiles)) {
  source(file.path('R', 'aggregator', aggfiles[s])) 
}
```

```{r}
source(file.path('R', 'spatial_helpers.R'))
```

# User setting

This stuff will likely end up in the outer aggregation control file. They could be passed as arguments, or controlled here.

I'm developing separate files for theme_agg and spatial_agg, but we will want to have a version that does both interleaved.

## Set paths

Set the path to the scenario outputs from the EWR. **TODO** Can we make this interactive to allow people to use this without typing in paths? This could also be passed directly from the scenario controller if we want to give it master control.

Need to clean this up

```{r}
ScenarioFolder <- file.path('data', 'testsmall')
EWRresults <- file.path(ScenarioFolder, 'Output_files', 'EWR')
gpath <- file.path('data', 'bom_gauges.shp')
ewr_causal_path <- file.path('data', 'causal_networks', 'causal_ewr.rds')
```

### Gauge for demo and detailed looks

```{r}
# Dubbo is '421001', has 24 EWRs
# Warren Weir is '421004', has 30 EWRs. 
example_gauge <- '421001'
```

### Quick look at the incoming data

The EWR results come in two flavours- summary over the span of the run and annual. We can read them in and look at them with `get_ewr_output` which read them in and does some cleanup. Typically we would feed the path to `theme_agg_multi` which would read them in internally (see below) to avoid having the objects in memory. Note that `get_ewr_output` has additional arguments to filter by gauge and scenario on read-in, allowing parallelisation without overloading memory.

I could use `prep_ewr_agg`, but that wraps `get_ewr_output` and making it geographic. The geography doesn't need to happen here since we're just doing theme. Doesn't really matter either way.

The summary data

```{r}
sumdat <- get_ewr_output(EWRresults, type = 'summary')
# Would make it geographic:
# sumdat <- prep_ewr_agg(EWRresults, type = 'summary', geopath = gpath)
sumdat
```

And the annual-

**TODO** not currently working- bug in EWR

```{r}
#| eval: false
anndat <- get_ewr_output(EWRresults, type = 'annual')
anndat
```

## Set aggregation parameters

Here, we want to set the parameters for each step of the aggregation. We need to set the sequence of aggregation steps and the aggregation functions to apply at each step. We also need the `causal_rels` list of relationships between theme levels. The dataframes in this list are currently in `Causal_networks/data` and I'm repackaging into a list. We should, however automate that when we make `Causal_networks` a package, so we can just access that list by loading the package and not need to specify. Will need to have different lists for different themes as they come online.

Note that the `aggseq` list needs to be nested, though if a relationship exists levels can be jumped (e.g. we could go straight from `env_obj` to `target_20_year_2039`.

The `aggseq` list will typically need to start with `ewr_code_timing` and aggregate from there into `ewr_code`. A similar approach could be done if we want to lump the `ewr_code`s themselves, e.g. put EF4a,b,c,d into EF4. I'm using CompensatingFactor as teh aggregation function for that step here, assuming that passing either timing sub-code means the main code passes.

The `funseq` is a list instead of a simple vector because multiple functions can be used at each step. When multiple functions are passed, though, they are factorial (each function is calculated on the results of all previous aggregations). And so it is likely that we would run the aggregation multiple times if we want to look at specific combinations.

As a demonstration, I've set a range of theme levels that hit all the theme relationship dataframes from the causal networks, and set the aggregation functions fairly simply, but with two multi-aggregation steps to illustrate how that works.

We want the *possibility* to set these as arguments, but for most runs, they will take default values.

```{r}
aggseq <- list(c('ewr_code_timing', 'ewr_code'),
               c('ewr_code', "env_obj"), 
             c('env_obj', "Specific_goal"), 
             c('Specific_goal', 'Objective'), 
             c('Objective', 'target_5_year_2024'))

funseq <- list(c('CompensatingFactor'),
               c('ArithmeticMean', 'LimitingFactor'),
             c('ArithmeticMean', 'LimitingFactor'),
             c('ArithmeticMean'),
             c('ArithmeticMean'))
```

We can either pass the input data as a dataframe (e.g. a dataframe of EWR outputs), or a path to a directory, in which case `get_ewr_output` goes and reads in the data.

I originally wrote `get_ewr_output` to automatically get both the annual and summary ewr output files, but I think given a typical workflow it will make more sense to just wrap it if we want to do that. Even if we want to use both, they won't talk to each other for any of the processing steps, so might as well run in parallel.

The `gaugefilter` and `scenariofilter` arguments are useful if we're only interested in some gauges or scenarios, but I ignore them until [Parallelisation]. Mostly, I developed them so we can easily parallelise this processing by feeding different sets of gauges or scenarios to this function in parallel.

The `groupers` and `aggCols` arguments can take a number of different formats- character vectors, tidyselect syntax (e.g. `starts_with('ewr')`, or bare column names. I take advantage of this in `theme_agg_mutli` a bit, but it gives the user quite a few options for specifying the columns to use. The use of `selectcreator` makes it robust to nonexistent columns.

```{r}
#| message: false
#| warning: false
simpleThemeAgg <- multi_aggregate(dat = sumdat,
                         causal_edges = make_edges(ewr_causal_path, aggseq),
                         groupers = c('scenario', 'gauge'),
                         aggCols = 'ewr_achieved',
                         aggsequence = aggseq,
                         funsequence = funseq)
simpleThemeAgg
```

That has 4 columns of output values because the second step found the `ArithmeticMean` and `LimitingFactor` for `ewr_achieved` into `env_obj` and then the third step found the `ArithmeticMean` and `LimitingFactor` for each of those outcomes into `Specific_goal`. Each subsequent step only found the `ArithmeticMean` for each, and so the number of output columns stopped growing.

The naming of the output columns is unweildy, but describes exactly what the numbers are. For example, the last column is

```{r}
names(simpleThemeAgg)[ncol(simpleThemeAgg)]
```

This says the values in this column are the 5-year targets, calculated as the arithmetic mean of Objectives, which were the arithmetic mean of Specific goals, which were calculated from env_obj as limiting factors, which were obtained from the ewrs as limiting factors and those were calculated from the ewr timings as compensating factors.

It may be easier to think about this from the other direction- `ewr_achieved` were aggregated from `ewr_code_timing` into `ewr_code` as Compensating Factors, then into `env_obj` as limiting factors- for the env_obj to pass, all ewrs contributing to it must pass. Then the `env_obj`s were aggregated into `Specific_goal`, again as limiting factors, so to meet a goal, all contributing `env_obj` must pass. Those specific goals were then aggregated into `Objectives` with the arithmetic mean, so the value for an `Objective` is then the average of the contributing `Specific_goal`s. Since in this example the `Specific_goals` will be either 1 or 0, this average gives the proportion of `Specific_goal`s that are met for each `Objective`. Similarly, the 5-year targets were obtained by averaging the `Objective`s contributing to them.

A different way to track the aggregations is possible by including them in columns instead of the names. This takes more memory, but can be clearer and makes subsequent uses easier in many cases. For memory purposes, I currently just parse the names into columns post-hoc. If we develop the ability to do different aggregations on different groups within a node type, we will need to make the columns as we go (and we will have to use the column method to track, since different things will happen to different values in a column).

In the example above, we can feed the output data to `agg_names_to_cols`,

```{r}
#| message: false
#| 
agg_names_to_cols(simpleThemeAgg, aggsequence = aggseq, funsequence = funseq, aggCols = 'ewr_achieved')
```

In practice, what makes most sense is to use a switch inside `theme_agg_multi` to return this format.

```{r}
#| message: false

simpleColHistory <-  multi_aggregate(dat = sumdat,
                         causal_edges = make_edges(ewr_causal_path, aggseq),
                         groupers = c('scenario', 'gauge'),
                         aggCols = 'ewr_achieved',
                         aggsequence = aggseq,
                         funsequence = funseq,
                         namehistory = FALSE)
simpleColHistory
```

# Additional demonstrations

The above is just a simple example of how to use the aggregation function, but there are more things we can do with it.

## Arbitrary input data

There are two main types of input EWR data (summary and annual), but we expect there will be any number of input datasets once other modules exist. Provided the causal relationships are defined for the input sets, the specific input datasets and columns of data to aggregate are general. For example, we can feed it the annual data (`type = 'annual'`) and aggregate two different columns (feed `aggCols` a vector of column names). In the case of the annual data, we might also want to group by year in addition to gauge and scenario, and so we add *year* to the `groupers` vector. We'll keep the same sequences of aggregation and functions, but noting that they now are calculated for both `aggCols`.

**TODO** turn back on once EWR bug fixed

```{r}
#| message: false
#| warning: false
#| eval: false

annualColHistory <-  multi_aggregate(dat = anndat,
                                     causal_edges = make_edges(ewr_causal_path,
                                                               aggseq),
                                     groupers = c('scenario', 'gauge', 'year'),
                                     aggCols = c('num_events', 'event_length'),
                                     aggsequence = aggseq,
                                     funsequence = funseq,
                                     namehistory = FALSE)
annualColHistory
```

## Returning every stage

By default, we return only the final theme aggregation after stepping up the full sequence set by `aggsequence`. But in some cases, we might want to return all of the intermediate aggregations (e.g. at each step of `aggsequence`). This full aggregation sequence can be useful for testing and checking, but probably more importantly, allows using each step to be fed as colour or other attributes to the causal network. In this case, we pass `saveintermediate = TRUE` to `theme_agg_multi`, and it returns a list of tibbles named by the theme level instead of a single final tibble. We cannot just attach the stage results as columns to a flat df because the aggregation is many-to-many, and so the rows do not match and are not strictly nested. Thus, each stage needs its own dataframe to avoid duplicating or deleting data.

```{r}
#| message: false
# allsteps <- theme_agg_multi(EWRresults,
#                             type = 'summary',
#                             causal_rels = ewr_causal_list,
#                             aggsequence = aggseq,
#                             funsequence = funseq,
#                             aggCols = 'ewr_achieved',
#                             groupers = c('scenario', 'gauge'),
#                             saveintermediate = TRUE,
#                             namehistory = FALSE)

allsteps  <-  multi_aggregate(dat = sumdat,
                         causal_edges = make_edges(ewr_causal_path, aggseq),
                         groupers = c('scenario', 'gauge'),
                         aggCols = 'ewr_achieved',
                         aggsequence = aggseq,
                         funsequence = funseq,
                         saveintermediate = TRUE,
                         namehistory = FALSE)

names(allsteps)
```

### Causal plot

By returning values at each stage, we can map those to colour (and later size) in a causal network. In practice, this will happen in the Comparer (and the initial setup data arrangement will be made into a function there), but we can demonstrate it quickly here. I'll put the values on the nodes, and so change their colour. To do this, I'll follow the usual causal_plots approach of making edges and nodes, and then use a join to attach the value to each node. This approach should be straightforward to make a function that we can call in the Comparer module.

I'm limiting the edge creation to a single gauge, and so will filter the theme aggregations accordingly (or just rely on the join to drop). The `ewr_node_timing` is likely just confusing to include here, so we cut it out of the fromtos.

```{r}
edges <- make_edges(ewr_causal_path, 
                    fromtos = aggseq[2:length(aggseq)],
                    gaugefilter = example_gauge)
  # make_edges(dflist = ewr_causal_list, 
  #              fromtos = aggseq[2:length(aggseq)],
  #              gaugefilter = example_gauge)

# 
nodes <- make_nodes(edges)
```

Most of what happens here needs a lightweight wrapper to make it a function in the Comparer module- basically do the `extract_vals` and the `join`, and possibly roll straight into the plot.

**TODO** clean up the targetlevels by default. They'll be needed especially once we have spatial, here mostly around the first level.

```{r}
#| message: false
#| warning: false

# set up to make a function
# need to grab the right set of aggregations if there are multiple at some stages
whichaggs <- c('CompensatingFactor',
               'ArithmeticMean',
               'ArithmeticMean',
               'ArithmeticMean',
               'ArithmeticMean')

# What is the column that defines the value?
valcol <- 'ewr_achieved'

# Get the values for each node
targetlevels <- names(allsteps)
targetlevels[1] <- 'ewr_code_timing'
aggvals <- extract_vals_causal(allsteps, whichaggs, valcol, 
                               targetlevels = targetlevels)

# Cut off the ewr_code_timing- should really have a drop_step argument to just not return it? In the extract_vals_causal
aggvals <- aggvals %>% filter(NodeType != 'ewr_code_timing')
# cut to relevant gauge, then remove- causes problems since node levels above env_obj aren't gauge-referenced
aggvals <- aggvals %>% filter(gauge == example_gauge) %>% 
  select(-gauge)

# join to the nodes
nodes_with_vals <- left_join(nodes, aggvals)
```

Use those to make the plot. Clip to just the base scenario for now. Colours are the proportion passing, e.g. Arithmetic Means at every step. Light yellow is 1, dark purple is 0.

```{r}

aggNetwork <- make_causal_plot(nodes = filter(nodes_with_vals, 
                                        scenario == 'base'),
                 edges = edges,
                 edge_pal = 'black',
                 node_pal = list(value = 'scico::tokyo'),
                 node_colorset = 'ewr_achieved',
                 render = FALSE)

DiagrammeR::render_graph(aggNetwork)
```

This is really getting into the purview of the **Comparer**, where we will do things like calculate differences, but an obvious thing to do here is to make these networks for the different scenarios. For example, in the increased watering scenario, we see a lot more light colours, and so better performance across the range of outcomes.

```{r}
aggNetwork <- make_causal_plot(nodes = filter(nodes_with_vals, 
                                        scenario == 'up4'),
                 edges = edges,
                 edge_pal = 'black',
                 node_pal = list(value = 'scico::tokyo'),
                 node_colorset = 'ewr_achieved',
                 render = FALSE)

DiagrammeR::render_graph(aggNetwork)
```

## User-set functions

We have established a simple set of default aggregation functions (`ArithmeticMean`, `GeometricMean`, `LimitingFactor`, and `CompensatingFactor`), available in `default_agg_functions`. I expect that list to grow, but it is also possible to ad-hoc define functions as a user and include in `funsequence`. Previously, we have used this sort of approach for things like threshold functions. For example, we might want to know the mean event length for events longer than 2 days for the ewr to objective aggregation, and thereafter scale up with `ArithmeticMean`.

```{r}
event2 <- function(x) {
  mean(ifelse(x > 2, x, NA), na.rm = TRUE)
}

newfuns <- list(c('CompensatingFactor'),
                  c('event2'),
             c('event2'),
             c('ArithmeticMean'),
             c('ArithmeticMean'))
```

**TODO** turn back on

```{r}
#| message: false
#| warning: false
#| eval: false
annualEv2 <-  multi_aggregate(dat = anndat,
                         causal_edges = make_edges(ewr_causal_path, aggseq),
                         groupers = c('scenario', 'gauge', 'year'),
                         aggCols = 'event_length',
                         aggsequence = aggseq,
                         funsequence = newfuns,
                         namehistory = FALSE)
# lots of NaN because many years and locations didn't have events > 2, so for ease of viewing, filter
annualEv2 %>% filter(!is.nan(event_length))
```

## Gauge and scenario -filtering

Reading in all of the EWR results across all gauges and scenarios could be massive, depending on the spatial scale and the number of scenarios. We also just might only be interested in some subset for things like plotting. To address this, `get_ewr_output` has `gaugefilter` and `scenariofilter` arguments. I originally had that in `theme_agg_multi` itself, but now that theme and spatial use the same `multi_` function, it makes more sense to use a wrapper that does the read-in and agg. If we wanted to filter the input data once it's in memory, it would be relatively trivial to put filter options in `multi_aggregate` or, more appropriately, filter the data being fed into its `dat` argument. The `gaugefilter` and `scenariofilter` arguments to `get_ewr_output` have a different goal- only read in and process a subset of the data. This will be particularly useful once we have lots of data that doesn't fit in memory or want to parallel process.

As a brief example, we might only want the baseline scenario for gauges `example_gauge` and 421004.

If we have *all* the data in memory already, we can just pipe it in through a filter (or filter the first argument).

```{r}
#| message: false
smallthemeagg  <-  sumdat %>% 
  filter(gauge %in% c(example_gauge, '421004') & scenario == 'base') %>% 
  multi_aggregate(causal_edges = make_edges(ewr_causal_path,
                                            aggseq),
                  groupers = c('scenario', 'gauge'),
                  aggCols = 'ewr_achieved',
                  aggsequence = aggseq,
                  funsequence = funseq,
                  namehistory = FALSE)

table(smallthemeagg$gauge, smallthemeagg$scenario)
```

But if we want to only read-in the relevant data, we use the `read_and_agg` wrapper. The `gaugefilter` argument only works (currently) if there are separate files for each gauge. *TODO*- once we settle on a data format, I should be able to write the gaugefilter differently to only read the desired gauge from the file.

```{r}
#| message: false

smallreadagg  <-  read_and_agg(datpath = EWRresults, type = 'summary',
                               geopath = gpath,
                               causalpath = ewr_causal_path,
                               groupers = c('scenario', 'gauge'),
                               aggCols = 'ewr_achieved',
                               aggsequence = aggseq,
                               funsequence = funseq,
                               namehistory = FALSE, 
                               gaugefilter = NULL,
                               scenariofilter = 'base')

table(smallreadagg$gauge, smallreadagg$scenario)
```

So, that's the same as above, but slower, since the read-in happens first. The advantage comes in if we don't want the original data sitting in memory, such as parallelisation.

Note, too, that if we just don't want the input data sitting in the environment, we can use `read_and_agg` with no `*filter` arguments, in which case it works just like `multi_aggregate`, but takes paths instead of objects as arguments. For example, saving all intermediate and no filtering-

```{r}
#| message: false
readallsteps <- read_and_agg(datpath = EWRresults, type = 'summary',
                             geopath = gpath,
                             causalpath = ewr_causal_path,
                             groupers = c('scenario', 'gauge'),
                             aggCols = 'ewr_achieved',
                             aggsequence = aggseq,
                             funsequence = funseq,
                             saveintermediate = TRUE,
                             namehistory = FALSE)

names(readallsteps)
```

### Parallelisation

The gauge and scenario filtering gives an easy way to parallelise. I haven't written this into a function yet (likely would be a wrapper around `read_and_agg`, but could be incorporated into `read_and_agg` itself, but we can demo how it works here. Parallelisation will not only speed up the processing, but because we can do the data reads inside the function, parallelisation over gauges and scenarios avoids reading all the data in at once and so can avoid smashing memory.

We'd obviously need to get the gauge and scenario lists from somewhere other than previously-read data, but that's easy. And we'd want to be smarter about the size of the chunks we're processing than just doing each gauge-scenario combo in its own thread, but again, that's not hard. A bit of nitty-gritty: `future` is supposed to handle the `.export` from the calling environment, and seems to do just fine with everything except the aggregation functions. That can happen with nested `foreach` inside functions, but I think here it might be happening because of the way we're using `{{}}` to pass an arbitrary set of functions. Easy enough to fix, by passing something to `.export`, but annoying.

This creates the same thing as `simpleThemeAgg`, but done in parallel. Which is slower for the toy, but has the potential to be much faster.

The current data format has changed a bit, so not using a gauge loop here for the moment until we sort out file formats. Requires a bit of a hack, but not worth spending time on it until we have a format.

```{r}
#| message: false
#| warning: false

registerDoFuture()
plan(multisession)

allgauges <- 'all' # unique(simpleThemeAgg$gauge)
allscenes <- unique(simpleThemeAgg$scenario)

passfuns <- unique(unlist(funseq))

parThemeAgg <- foreach(s = allscenes, 
                       .combine = bind_rows,
                       .export = passfuns) %:%
  foreach(g = allgauges, 
          .combine = bind_rows) %dopar% {
    
            # annoying hack to not have a gaugefilter but keep the loop for later
            if (g == 'all') {
              g <- NULL
            }
            
    read_and_agg(datpath = EWRresults, type = 'summary',
                 geopath = gpath,
                 causalpath = ewr_causal_path,
                 groupers = c('scenario', 'gauge'),
                 aggCols = 'ewr_achieved',
                 aggsequence = aggseq,
                 funsequence = funseq,
                 namehistory = TRUE, 
                 gaugefilter = g,
                 scenariofilter = s)
  }

parThemeAgg
```

If we're doing something here that is too big to return the full output, it would also be straightforward for the parallel loop to save the iterations and not return anything. Then we could read the output in in pieces into the comparer. There's also the possibility of using things like arrow to handle analysis of data too big for memory.

# Still to do/next steps

-   tests

    -   Robustness of aggCols and groupers to different tidyselect, bare names, etc

    -   Robustness of functions to different formats

    -   Everything else- formats, missing columns, etc

-   Establish defaults

-   Lumped EWR codes (EF4a,b,c, --\> EF4).

    -   Would just be adding a column and aggregation step

-   Different aggregation for different groups of values (e.g. bird breeding using min(), fish persistence using mean()).

    -   Need to think about the best way to do this. It has the potential to be a very complex set of mappings. I could do something like how i assigned different colour palettes to different groups in the causal plotting.

    -   Or we could chop the data up and do them one at a time.

    -   The history tracking would have to go in columns in this case, and have to be generated on the fly, not post-hoc from the names.

-   Save metadata, including the funlists etc.

-   re-write `gaugefilter` and `scenariofilter` once we have a data format so we can leave the bits we don't want on-disk.

-   Cleaner way of dealing with the input test data. Probably in `data/`, or `data-raw/`, but need to coordinate with the scenario controller and `hydrotesting`

-   Actual input data should be pointed to with a config file.

-   Cleaner causal data- have the causal `data/` have the list instead of the tables, and access it through that.

-   Parallelisation integrated into the flow

-   Overarching beginning to end flow notebook/script
